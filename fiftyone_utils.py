# fiftyone_utils.py

import fiftyone as fo
import fiftyone.zoo as foz
import fiftyone.core.labels as fol
from config import LOW_CONF_THRESHOLD
from tqdm import tqdm

import random

BDD100K_CLASSES = [
    "bike", "bus", "car", "motor", "person",
    "rider", "traffic light", "traffic sign", "train", "truck"
]

def add_dummy_predictions(dataset, num_preds=3):
    for sample in tqdm(dataset):
        detections = []
        for _ in range(num_preds):
            label = random.choice(BDD100K_CLASSES)
            bbox = [
                round(random.uniform(0.2, 0.6), 2),  # x
                round(random.uniform(0.2, 0.6), 2),  # y
                round(random.uniform(0.1, 0.3), 2),  # w
                round(random.uniform(0.1, 0.3), 2)   # h
            ]
            confidence = round(random.uniform(0.2, 0.9), 2)

            detections.append(fol.Detection(
                label=label,
                bounding_box=bbox,
                confidence=confidence,
            ))

        sample["predictions"] = fol.Detections(detections=detections)
        sample.save()


def built_in_bbk100k(source_local_dir):
    # The path to the source files that you manually downloaded
    dataset = foz.load_zoo_dataset(
        "bdd100k",
        source_dir=source_local_dir,
    )
    session = fo.launch_app(dataset)


def setup_dataset(fiftyone_dataset_zoo_name_or_url, source_local_dir, save_local_dataset_name):
    # 1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë“±ë¡
    print(f"ğŸ“¦ {fiftyone_dataset_zoo_name_or_url} ë°ì´í„°ì…‹ì„ ë¡œë”© ì¤‘...")
    dataset = foz.load_zoo_dataset(
        fiftyone_dataset_zoo_name_or_url,
        source_dir=source_local_dir,
        dataset_name=save_local_dataset_name,
        persistent=True,
        overwrite=True,
    )

    print(f"ë“±ë¡ ì™„ë£Œ: {dataset.name}")
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")


def preview_samples(dataset_name, max_samples=3):
    dataset = fo.load_dataset(dataset_name)

    print(f"\n'{dataset_name}' ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:")
    for sample in dataset[:max_samples]:
        print("íŒŒì¼:", sample.filepath)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ:", sample.field_names)

        # ë¼ë²¨ í•„ë“œ ìë™ íƒìƒ‰ ë° ê°œìˆ˜ ì¶œë ¥
        label_fields = [f for f in sample.field_names if isinstance(sample[f], fol.Detections)]
        if label_fields:
            for field in label_fields:
                print(f"ë¼ë²¨ í•„ë“œ: '{field}', ê°ì²´ ìˆ˜: {len(sample[field].detections)}")
        else:
            print("ë¼ë²¨ ì—†ìŒ")

        print("-" * 40)


def launch_dataset_gui(dataset_name):
    dataset = fo.load_dataset(dataset_name)
    print(f"\n{dataset_name} GUI ì‹¤í–‰ ì¤‘...")
    session = fo.launch_app(dataset)
    session.wait()




def load_filtered_samples(dataset_name):
    dataset = fo.load_dataset(dataset_name)

    # ëª¨ë“  ìƒ˜í”Œì— 'predictions' í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    predictions_exist = all("predictions" in sample.field_names for sample in dataset)

    if not predictions_exist:
        print("'predictions' í•„ë“œê°€ ì—†ì–´ dummy ì˜ˆì¸¡ì„ ì¶”ê°€í•©ë‹ˆë‹¤...")
        add_dummy_predictions(dataset)

    # 'confidence'ê°€ ì—†ëŠ” ê²½ìš°ë„ ëŒ€ë¹„
    view = dataset.filter_labels(
        "predictions",
        (fo.ViewField("confidence") < LOW_CONF_THRESHOLD) & (fo.ViewField("confidence") != None)
    )

    return view



def get_local_dataset_lists():
    return fo.list_datasets()


def get_local_dataset_infor(local_dataset_name):
    dataset = fo.load_dataset(local_dataset_name)
    return dataset


def delete_local_dataset(local_dataset_name):
    fo.delete_dataset(local_dataset_name)


def verified_local_dataset():
    for local_dataset_name in get_local_dataset_lists():
        data = get_local_dataset_infor(local_dataset_name=local_dataset_name)
        if len(data) == 0:
            delete_local_dataset(local_dataset_name)


if __name__ == "__main__":
    # verified_local_dataset()

    print(get_local_dataset_lists())
    launch_dataset_gui('bdd100k_custom')
